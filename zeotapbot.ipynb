{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "s0fuYSEDWeDp",
   "metadata": {
    "executionInfo": {
     "elapsed": 5992,
     "status": "ok",
     "timestamp": 1740663148412,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "s0fuYSEDWeDp"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from google import genai \n",
    "from bs4 import BeautifulSoup\n",
    "import requests.compat\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Ng662rPMZV4q",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1740663151728,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "Ng662rPMZV4q"
   },
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "client = genai.Client(api_key=\"AIzaSyAnb4Nx6pi82_vyJnxhhq5Il6CnE9GcWdQ\")\n",
    "# genai.configure(api_key=\"AIzaSyAnb4Nx6pi82_vyJnxhhq5Il6CnE9GcWdQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "g1mBU0EEmio9",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740663156427,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "g1mBU0EEmio9"
   },
   "outputs": [],
   "source": [
    "def crawl_docs(base_url, allowed_prefix, visited=None, max_depth=2, depth=0, max_links=5):\n",
    "    \"\"\"\n",
    "    Recursively crawls pages starting at base_url.\n",
    "\n",
    "    For each page:\n",
    "      1. Gets the entire HTML (response.text) and adds it to content.\n",
    "      2. Finds all links on that page whose URL starts with allowed_prefix and that have not been visited.\n",
    "      3. Limits the links to max_links.\n",
    "      4. First recurses into the first subroute link encountered before processing remaining links.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if depth > max_depth:\n",
    "        return \"\"\n",
    "\n",
    "    content = \"\"\n",
    "    try:\n",
    "        print(f\"Crawling: {base_url}\")\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return \"\"\n",
    "\n",
    "        # Save the full HTML content of the current page.\n",
    "        current_html = response.text\n",
    "        visited.add(base_url)\n",
    "\n",
    "        # Parse the page to find subroute links.\n",
    "        soup = BeautifulSoup(current_html, \"html.parser\")\n",
    "\n",
    "        # Extract relevant content\n",
    "        page_data = [tag.get_text(strip=True) for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"])]\n",
    "        content += \"\\n\".join(page_data) + \"\\n\"\n",
    "\n",
    "        valid_links = []\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            # Resolve relative URLs.\n",
    "            if not href.startswith(\"http\"):\n",
    "                href = requests.compat.urljoin(base_url, href)\n",
    "            # Only add links that match the allowed prefix and have not been visited.\n",
    "            if href.startswith(allowed_prefix) and href not in visited:\n",
    "                valid_links.append(href)\n",
    "\n",
    "        valid_links = valid_links[:max_links]\n",
    "\n",
    "        if valid_links:\n",
    "            # First, recursively follow the FIRST subroute encountered.\n",
    "            content += crawl_docs(valid_links[0], allowed_prefix, visited, max_depth, depth + 1, max_links)\n",
    "            # Then process any remaining subroute links.\n",
    "            for link in valid_links[1:]:\n",
    "                content += crawl_docs(link, allowed_prefix, visited, max_depth, depth + 1, max_links)\n",
    "                time.sleep(0.5)  # be polite to the server\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {base_url}: {e}\")\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "}\n",
    "def fetch_html(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        return [tag.get_text(strip=True) for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"])]\n",
    "    else:\n",
    "        return f\"Failed to fetch data. Status Code: {response.status_code}\"\n",
    "    \n",
    "url = \"https://segment.com/docs/getting-started/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cAKUh_4-mmi7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74836,
     "status": "ok",
     "timestamp": 1740663238883,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "cAKUh_4-mmi7",
    "outputId": "abf79fe0-c5a6-42af-f594-c3410b91a487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://segment.com/docs/\n",
      "Failed to fetch https://segment.com/docs/\n",
      "Crawling: https://docs.mparticle.com/guides/\n",
      "Crawling: https://docs.mparticle.com/guides/platform-guide/\n",
      "Crawling: https://docs.mparticle.com/guides/glossary/\n",
      "Crawling: https://docs.mparticle.com/guides/getting-started/create-an-input/\n",
      "Crawling: https://docs.mparticle.com/guides/getting-started/start-capturing-data/\n",
      "Crawling: https://docs.mparticle.com/guides/getting-started/connect-an-event-output/\n",
      "Crawling: https://docs.mparticle.com/guides/getting-started/create-an-audience/\n",
      "Crawling: https://docs.mparticle.com/guides/platform-guide/activity/\n",
      "Crawling: https://docs.mparticle.com/guides/getting-started/connect-an-audience-output/\n",
      "Crawling: https://docs.mparticle.com/guides/getting-started/transform/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/introduction/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/profiles/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/audiences/overview/\n",
      "Crawling: https://docs.mparticle.com/guides/platform-guide/connections/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/audiences/create-an-audience/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/audiences/connect-an-audience/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/audiences/manage-audiences/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/audiences/real-time/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/audiences/standard/\n",
      "Crawling: https://docs.mparticle.com/guides/platform-guide/data-filter/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/calculated-attributes/overview/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/calculated-attributes/using-calculated-attributes/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/calculated-attributes/generate-with-ai/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/calculated-attributes/reference/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/predictive-audiences/overview/\n",
      "Crawling: https://docs.mparticle.com/guides/platform-guide/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/predictive-audiences/using-predictive-audiences/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/journeys/overview/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/journeys/use-journeys/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/journeys/download/\n",
      "Crawling: https://docs.mparticle.com/guides/personalization/journeys/ab-testing/\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart#content\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-3-install-lytics\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-2-content-setup\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-4-personalized-message\n",
      "Crawling: https://docs.lytics.com/docs/data\n",
      "Crawling: https://docs.lytics.com/docs/developer-attributes\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-3-install-lytics\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-3-install-lytics#content\n",
      "Crawling: https://docs.lytics.com/docs/developer-segments\n",
      "Crawling: https://docs.lytics.com/docs/lead-capture\n",
      "Crawling: https://docs.lytics.com/docs/lead-capture\n",
      "Crawling: https://docs.lytics.com/docs/guide-content-recommendations\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-2-content-setup\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-2-content-setup#content\n",
      "Crawling: https://docs.lytics.com/docs/what-is-vault\n",
      "Crawling: https://docs.lytics.com/docs/account-details\n",
      "Crawling: https://docs.lytics.com/docs/monitoring-and-alerts\n",
      "Crawling: https://docs.lytics.com/docs/job-alerts\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-4-personalized-message\n",
      "Crawling: https://docs.lytics.com/docs/developer-quickstart-4-personalized-message#content\n",
      "Crawling: https://docs.lytics.com/docs/monitoring-metrics-copy\n",
      "Crawling: https://docs.lytics.com/docs/monitoring-job-status\n",
      "Crawling: https://docs.lytics.com/docs/monitoring-lytics\n",
      "Crawling: https://docs.lytics.com/docs/usage-metrics\n",
      "Crawling: https://docs.lytics.com/docs/data\n",
      "Crawling: https://docs.lytics.com/docs/data#content\n",
      "Crawling: https://docs.lytics.com/docs/account-users\n",
      "Crawling: https://docs.lytics.com/docs/single-sign-on-overview\n",
      "Crawling: https://docs.lytics.com/docs/account-settings\n",
      "Crawling: https://docs.lytics.com/docs/account-details-1\n",
      "Crawling: https://docs.zeotap.com/home/en-us/\n",
      "Crawling done\n"
     ]
    }
   ],
   "source": [
    "segment_base = \"https://segment.com/docs/\"\n",
    "mparticle_base = \"https://docs.mparticle.com/guides/\"\n",
    "lytics_base = \"https://docs.lytics.com/docs/\"\n",
    "zeotap_base = \"https://docs.zeotap.com/home/en-us/\"\n",
    "segment_docs = crawl_docs(segment_base, segment_base, max_depth=2, max_links=5)\n",
    "mparticle_docs = crawl_docs(mparticle_base, mparticle_base, max_depth=2, max_links=5)\n",
    "lytics_docs = crawl_docs(\"https://docs.lytics.com/docs/developer-quickstart\", lytics_base, max_depth=2, max_links=5)\n",
    "zeotap_docs = crawl_docs(zeotap_base, zeotap_base, max_depth=1, max_links=2)\n",
    "print(\"Crawling done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Q-M--KedndvM",
   "metadata": {
    "executionInfo": {
     "elapsed": 2991,
     "status": "ok",
     "timestamp": 1740663360627,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "Q-M--KedndvM"
   },
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(mparticle_docs, \"html.parser\")\n",
    "# sdata = [tag.get_text(strip=True) for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aaad27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to save for segment_data.txt\n",
      "Data saved to mparticle_data.txt\n",
      "Data saved to lytics_data.txt\n",
      "Data saved to zeotap_data.txt\n"
     ]
    }
   ],
   "source": [
    "def save_data_to_file(data, filename):\n",
    "    if data:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(data)\n",
    "            print(f\"Data saved to {filename}\")\n",
    "    else:\n",
    "        print(f\"No data to save for {filename}\")\n",
    "\n",
    "# Save each website's data separately in the root of the repo\n",
    "save_data_to_file(segment_docs, \"segment_data.txt\")\n",
    "save_data_to_file(mparticle_docs, \"mparticle_data.txt\")\n",
    "save_data_to_file(lytics_docs, \"lytics_data.txt\")\n",
    "save_data_to_file(zeotap_docs, \"zeotap_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2-uSclDxZiDU",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740663365272,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "2-uSclDxZiDU"
   },
   "outputs": [],
   "source": [
    "\n",
    "scraped_data = sdata \n",
    "\n",
    "if scraped_data:\n",
    "    formatted_data = \"\\n\".join(scraped_data)  # Convert list to string\n",
    "else:\n",
    "    print(\"Failed to scrape data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nR6gHeRCZmH2",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740663482741,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "nR6gHeRCZmH2"
   },
   "outputs": [],
   "source": [
    "# Function to ask Gemini with strict limitations\n",
    "def ask_chatbot(question):\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a support chatbot that strictly answers based on the provided documentation.\n",
    "    If the question is unrelated or the answer is not in the data, respond with \"I can only provide information from the Segment documentation.\"\n",
    "\n",
    "    Documentation:\n",
    "    {formatted_data}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "VbmiZfzZZpQo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "executionInfo": {
     "elapsed": 6726,
     "status": "ok",
     "timestamp": 1740663594775,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "VbmiZfzZZpQo",
    "outputId": "59885235-ca87-4026-c1c1-965039f703f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In mParticle, an audience is a set of users who match a given set of criteria. When mParticle prepares to forward an audience, it is broken down into a series of messages about audience membership. Each message contains:\n",
      "\n",
      "mParticle then translates these messages into a format that can be read by each audience output partner, and forwards them via HTTP API. Each output deals with audience information a little differently, depending on their data structure, but there are two main patterns.\n",
      "\n",
      "Direct\n",
      "Some audience output partners allow mParticle to either to directly create an audience (some call them âlistsâ, or âsegmentsâ) via their API, or at least to manage the membership of an existing audience. The end result will be an âaudienceâ in the partner system, containing as many identities from the original mParticle audience as the output can accept. mParticle will continue to update the membership of the audience in the partner system as users are added and removed. Email marketing and social media platforms are usually in this category.\n",
      "\n",
      "Indirect\n",
      "Not all audience output services have a concept of âaudiencesâ that mParticle can map to. Others donât allow their audiences to be directly managed via API. In these cases, mParticle usually forwards audiences as some kind of user attribute or tag. Push messaging and other mobile-oriented services often fall into this category.\n",
      "\n",
      "As an example,Braze, has itâs own audience feature, called âSegmentsâ, but it does not allow mParticle to create segments via API. Instead, for each Braze-supported identity in the audience, mParticle sets a tag on the user, named after the audience. You can then easily find matching users in Braze by searching for that tag.\n",
      "\n",
      "The catch here is that it is often necessary for the output service to already have a record of the users you want to target. For this reason, this type of audience integration usually works best when paired with a matching event integration.\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "question = \"How audiences are forwarded? Info of its patterns?\"\n",
    "answer = ask_chatbot(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
