{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "s0fuYSEDWeDp",
   "metadata": {
    "executionInfo": {
     "elapsed": 5992,
     "status": "ok",
     "timestamp": 1740663148412,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "s0fuYSEDWeDp"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import google.generativeai as genai\n",
    "from bs4 import BeautifulSoup\n",
    "import requests.compat\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import html\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3a5894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "driver = webdriver.Chrome()\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "g1mBU0EEmio9",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740663156427,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "g1mBU0EEmio9"
   },
   "outputs": [],
   "source": [
    "def crawl_docs(base_url, allowed_prefix, visited=None, max_depth=None, depth=0, max_links=None):\n",
    "    \"\"\"\n",
    "    Recursively crawls pages starting at base_url.\n",
    "\n",
    "    For each page:\n",
    "      1. Loads the page using a driver and gets the rendered HTML.\n",
    "      2. Extracts text from h1, h2, h3, and p tags.\n",
    "      3. Finds and processes links starting with allowed_prefix, limiting them to max_links.\n",
    "      4. First recurses into the first valid link before processing the others.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if base_url in visited or (max_depth is not None and depth > max_depth):\n",
    "        return \"\"\n",
    "\n",
    "    visited.add(base_url)\n",
    "    print(f\"Crawling: {base_url}\")\n",
    "\n",
    "    content_parts = []\n",
    "    try:\n",
    "        driver.get(base_url)\n",
    "        time.sleep(0.5)\n",
    "        current_html = driver.page_source\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {base_url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {base_url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    decoded_html = html.unescape(current_html)\n",
    "    soup = BeautifulSoup(decoded_html, \"html.parser\")\n",
    "\n",
    "    page_data = [tag.get_text(strip=True) for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"])]\n",
    "    content_parts.append(\"\\n\".join(page_data))\n",
    "\n",
    "    valid_links = []\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = urljoin(base_url, link[\"href\"])\n",
    "        href = urlparse(href)._replace(fragment=\"\").geturl()\n",
    "        if not href.startswith(\"http\"):\n",
    "            href = requests.compat.urljoin(base_url, href)\n",
    "        if href.startswith(allowed_prefix) and href not in visited:\n",
    "            valid_links.append(href)\n",
    "            if max_links is not None and len(valid_links) >= max_links:\n",
    "                break\n",
    "\n",
    "    if valid_links:\n",
    "        content_parts.append(crawl_docs(valid_links[0], allowed_prefix, visited, max_depth, depth + 1, max_links))\n",
    "        for link in valid_links[1:]:\n",
    "            content_parts.append(crawl_docs(link, allowed_prefix, visited, max_depth, depth + 1, max_links))\n",
    "\n",
    "    return \"\\n\".join(content_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c2cf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "}\n",
    "def fetch_html(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        return [tag.get_text(strip=True) for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"])]\n",
    "    else:\n",
    "        return f\"Failed to fetch data. Status Code: {response.status_code}\"\n",
    "    \n",
    "url = \"https://segment.com/docs/getting-started/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cAKUh_4-mmi7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74836,
     "status": "ok",
     "timestamp": 1740663238883,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "cAKUh_4-mmi7",
    "outputId": "abf79fe0-c5a6-42af-f594-c3410b91a487"
   },
   "outputs": [],
   "source": [
    "segment_base = \"https://segment.com/docs/\"\n",
    "mparticle_base = \"https://docs.mparticle.com/guides/\"\n",
    "lytics_base = \"https://docs.lytics.com/docs/\"\n",
    "zeotap_base = \"https://docs.zeotap.com/home/en-us/\"\n",
    "segment_docs = crawl_docs(segment_base, segment_base, max_depth=None, max_links=None)\n",
    "# mparticle_docs = crawl_docs(mparticle_base, mparticle_base, max_depth=2, max_links=5)\n",
    "# lytics_docs = crawl_docs(\"https://docs.lytics.com/docs/developer-quickstart\", lytics_base, max_depth=None, max_links=None)\n",
    "# zeotap_docs = crawl_docs(zeotap_base, \"https://docs.zeotap.com/\", max_depth=None, max_links=None)\n",
    "print(\"Crawling done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaad27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_file(data, filename):\n",
    "    if data:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(data)\n",
    "            print(f\"Data saved to {filename}\")\n",
    "    else:\n",
    "        print(f\"No data to save for {filename}\")\n",
    "\n",
    "save_data_to_file(segment_docs, \"segment_data.txt\")\n",
    "save_data_to_file(mparticle_docs, \"mparticle_data.txt\")\n",
    "save_data_to_file(lytics_docs, \"lytics_data.txt\")\n",
    "save_data_to_file(zeotap_docs, \"zeotap_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2-uSclDxZiDU",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740663365272,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "2-uSclDxZiDU"
   },
   "outputs": [],
   "source": [
    "\n",
    "scraped_data = sdata \n",
    "\n",
    "if scraped_data:\n",
    "    formatted_data = \"\\n\".join(scraped_data)\n",
    "else:\n",
    "    print(\"Failed to scrape data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nR6gHeRCZmH2",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740663482741,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "nR6gHeRCZmH2"
   },
   "outputs": [],
   "source": [
    "def ask_chatbot(question):\n",
    "    model = genai.GenerativeModel(model_name='gemini-2.0-flash')\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a support chatbot that strictly answers based on the provided documentation.\n",
    "    If the question is unrelated or the answer is not in the data, respond with \"I can only provide information from the Segment documentation.\"\n",
    "\n",
    "    Documentation:\n",
    "    {formatted_data}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VbmiZfzZZpQo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "executionInfo": {
     "elapsed": 6726,
     "status": "ok",
     "timestamp": 1740663594775,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "VbmiZfzZZpQo",
    "outputId": "59885235-ca87-4026-c1c1-965039f703f0"
   },
   "outputs": [],
   "source": [
    "# Example query\n",
    "question = \"How audiences are forwarded? Info of its patterns?\"\n",
    "answer = ask_chatbot(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
