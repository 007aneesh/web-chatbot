{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "s0fuYSEDWeDp",
   "metadata": {
    "executionInfo": {
     "elapsed": 5992,
     "status": "ok",
     "timestamp": 1740663148412,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "s0fuYSEDWeDp"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from google import genai \n",
    "from bs4 import BeautifulSoup\n",
    "import requests.compat\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c3a5894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Ng662rPMZV4q",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1740663151728,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "Ng662rPMZV4q"
   },
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "client = genai.Client(api_key=\"AIzaSyAnb4Nx6pi82_vyJnxhhq5Il6CnE9GcWdQ\")\n",
    "# genai.configure(api_key=\"AIzaSyAnb4Nx6pi82_vyJnxhhq5Il6CnE9GcWdQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "g1mBU0EEmio9",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740663156427,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "g1mBU0EEmio9"
   },
   "outputs": [],
   "source": [
    "def crawl_docs(base_url, allowed_prefix, visited=None, max_depth=None, depth=0, max_links=None):\n",
    "    \"\"\"\n",
    "    Recursively crawls pages starting at base_url.\n",
    "\n",
    "    For each page:\n",
    "      1. Gets the entire HTML (response.text) and adds it to content.\n",
    "      2. Finds all links on that page whose URL starts with allowed_prefix and that have not been visited.\n",
    "      3. Limits the links to max_links.\n",
    "      4. First recurses into the first subroute link encountered before processing remaining links.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if max_depth is not None and depth > max_depth:\n",
    "        return \"\"\n",
    "\n",
    "    content = \"\"\n",
    "    try:\n",
    "        print(f\"Crawling: {base_url}\")\n",
    "        driver.get(base_url)\n",
    "        time.sleep(2)\n",
    "        headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "}\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return \"\"\n",
    "\n",
    "        # Save the full HTML content of the current page.\n",
    "        current_html = driver.page_source\n",
    "        visited.add(base_url)\n",
    "\n",
    "        decoded_html = html.unescape(current_html)\n",
    "\n",
    "        # Parse the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(decoded_html, \"html.parser\")\n",
    "\n",
    "        # Extract relevant content\n",
    "        page_data = [tag.get_text(strip=True) for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"])]\n",
    "        print(page_data)\n",
    "        content += \"\\n\".join(page_data) + \"\\n\"\n",
    "\n",
    "        valid_links = []\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            # Resolve relative URLs.\n",
    "            if not href.startswith(\"http\"):\n",
    "                href = requests.compat.urljoin(base_url, href)\n",
    "            # Only add links that match the allowed prefix and have not been visited.\n",
    "            if href.startswith(allowed_prefix) and href not in visited:\n",
    "                valid_links.append(href)\n",
    "\n",
    "        valid_links = valid_links[:max_links]\n",
    "\n",
    "        if valid_links:\n",
    "            # First, recursively follow the FIRST subroute encountered.\n",
    "            content += crawl_docs(valid_links[0], allowed_prefix, visited, max_depth, depth + 1, max_links)\n",
    "            # Then process any remaining subroute links.\n",
    "            for link in valid_links[1:]:\n",
    "                content += crawl_docs(link, allowed_prefix, visited, max_depth, depth + 1, max_links)\n",
    "                time.sleep(1)  # be polite to the server\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {base_url}: {e}\")\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c2cf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "}\n",
    "def fetch_html(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        return [tag.get_text(strip=True) for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"])]\n",
    "    else:\n",
    "        return f\"Failed to fetch data. Status Code: {response.status_code}\"\n",
    "    \n",
    "url = \"https://segment.com/docs/getting-started/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cAKUh_4-mmi7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74836,
     "status": "ok",
     "timestamp": 1740663238883,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "cAKUh_4-mmi7",
    "outputId": "abf79fe0-c5a6-42af-f594-c3410b91a487"
   },
   "outputs": [],
   "source": [
    "segment_base = \"https://segment.com/docs/getting-started/\"\n",
    "mparticle_base = \"https://docs.mparticle.com/guides/\"\n",
    "lytics_base = \"https://docs.lytics.com/docs/\"\n",
    "zeotap_base = \"https://docs.zeotap.com/home/en-us/\"\n",
    "# segment_docs = crawl_docs(segment_base, segment_base, max_depth=None, max_links=None)\n",
    "# mparticle_docs = crawl_docs(mparticle_base, mparticle_base, max_depth=2, max_links=5)\n",
    "# lytics_docs = crawl_docs(\"https://docs.lytics.com/docs/developer-quickstart\", lytics_base, max_depth=2, max_links=5)\n",
    "zeotap_docs = crawl_docs(zeotap_base, \"https://docs.zeotap.com/\", max_depth=2, max_links=5)\n",
    "print(\"Crawling done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Q-M--KedndvM",
   "metadata": {
    "executionInfo": {
     "elapsed": 2991,
     "status": "ok",
     "timestamp": 1740663360627,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "Q-M--KedndvM"
   },
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(mparticle_docs, \"html.parser\")\n",
    "# sdata = [tag.get_text(strip=True) for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaad27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_file(data, filename):\n",
    "    if data:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(data)\n",
    "            print(f\"Data saved to {filename}\")\n",
    "    else:\n",
    "        print(f\"No data to save for {filename}\")\n",
    "\n",
    "# Save each website's data separately in the root of the repo\n",
    "save_data_to_file(segment_docs, \"segment_data.txt\")\n",
    "save_data_to_file(mparticle_docs, \"mparticle_data.txt\")\n",
    "save_data_to_file(lytics_docs, \"lytics_data.txt\")\n",
    "save_data_to_file(zeotap_docs, \"zeotap_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2-uSclDxZiDU",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740663365272,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "2-uSclDxZiDU"
   },
   "outputs": [],
   "source": [
    "\n",
    "scraped_data = sdata \n",
    "\n",
    "if scraped_data:\n",
    "    formatted_data = \"\\n\".join(scraped_data)  # Convert list to string\n",
    "else:\n",
    "    print(\"Failed to scrape data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nR6gHeRCZmH2",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740663482741,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "nR6gHeRCZmH2"
   },
   "outputs": [],
   "source": [
    "# Function to ask Gemini with strict limitations\n",
    "def ask_chatbot(question):\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a support chatbot that strictly answers based on the provided documentation.\n",
    "    If the question is unrelated or the answer is not in the data, respond with \"I can only provide information from the Segment documentation.\"\n",
    "\n",
    "    Documentation:\n",
    "    {formatted_data}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VbmiZfzZZpQo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "executionInfo": {
     "elapsed": 6726,
     "status": "ok",
     "timestamp": 1740663594775,
     "user": {
      "displayName": "Aneesh Kumar",
      "userId": "03705977585329920413"
     },
     "user_tz": -330
    },
    "id": "VbmiZfzZZpQo",
    "outputId": "59885235-ca87-4026-c1c1-965039f703f0"
   },
   "outputs": [],
   "source": [
    "# Example query\n",
    "question = \"How audiences are forwarded? Info of its patterns?\"\n",
    "answer = ask_chatbot(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
